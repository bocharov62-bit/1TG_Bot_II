ОПИСАНИЕ ПРОЕКТА
TELEGRAM БОТ С ЛОКАЛЬНОЙ AI
===========================

НАЗНАЧЕНИЕ:
-----------
Telegram бот для ответов на вопросы пользователей с использованием 
локальной AI модели gemma3:1b через Ollama. Проект разработан для 
локального использования с возможностью развертывания через Docker.

ОСНОВНЫЕ ВОЗМОЖНОСТИ:
---------------------
✅ Отвечает на любые текстовые вопросы пользователей
✅ Использует локальную AI модель gemma3:1b (не требует интернета для AI)
✅ Краткие неформальные ответы (1-2 предложения)
✅ Простое развертывание через Docker Compose
✅ Автоматическое логирование всех операций
✅ Обработка ошибок с информативными сообщениями

ТЕХНОЛОГИЧЕСКИЙ СТЕК:
--------------------
- Python 3.11+ - основной язык разработки
- aiogram 3.x - фреймворк для Telegram бота
- Ollama - сервис для работы с локальными AI моделями
- gemma3:1b - легковесная AI модель (815 MB)
- Docker + Docker Compose - контейнеризация и оркестрация
- aiohttp - асинхронные HTTP запросы к Ollama

АРХИТЕКТУРА:
------------
Проект состоит из двух Docker контейнеров:

1. Контейнер Ollama (bot_primeta_ollama)
   - Сервис для работы с AI моделями
   - Порт: 11434 (внутренний)
   - Хранит загруженные модели

2. Контейнер Telegram бота (bot_primeta)
   - Python приложение с aiogram
   - Подключается к Ollama через внутреннюю Docker сеть
   - Обрабатывает сообщения от пользователей Telegram

СТРУКТУРА КОДА:
---------------
bot/
├── handlers/          # Обработчики сообщений Telegram
│   ├── start.py      # Обработка команды /start
│   └── messages.py   # Обработка текстовых сообщений
├── services/          # Бизнес-логика
│   └── ai_service.py # Сервис для работы с Ollama AI
├── utils/             # Вспомогательные модули
│   └── logger.py     # Настройка логирования
├── config.py          # Конфигурация (загрузка из .env)
└── main.py            # Точка входа приложения

ОСОБЕННОСТИ РЕАЛИЗАЦИИ:
-----------------------
- Модульная архитектура для легкой поддержки
- Асинхронная обработка запросов (asyncio)
- Детальное логирование всех операций
- Обработка ошибок с fallback сообщениями
- Ограничение длины ответов AI (1-2 предложения)
- Таймауты для предотвращения зависаний

ИСПОЛЬЗОВАНИЕ:
--------------
1. Получить токен бота у @BotFather в Telegram
2. Создать файл .env с токеном и настройками
3. Запустить контейнеры: docker-compose up -d
4. Загрузить модель: docker exec bot_primeta_ollama ollama pull gemma3:1b
5. Бот готов к работе!

Пользователь отправляет /start → получает приветствие
Пользователь задает вопрос → получает ответ от AI

ТРЕБОВАНИЯ К СИСТЕМЕ:
---------------------
- Docker Desktop (Windows/Mac) или Docker Engine (Linux)
- Минимум 2-3 GB свободной оперативной памяти
- ~1 GB свободного места на диске (для модели)
- Интернет для первоначальной загрузки модели

ПРЕИМУЩЕСТВА:
-------------
- Полная приватность (данные не отправляются в облако)
- Работа без постоянного интернета (после загрузки модели)
- Низкие требования к ресурсам
- Простое развертывание и настройка
- Открытый исходный код

ОГРАНИЧЕНИЯ:
------------
- Работает только локально (не 24/7 на сервере)
- Качество ответов зависит от модели gemma3:1b
- Время ответа: до 30 секунд
- Требует запущенного Docker

СТАТУС ПРОЕКТА:
---------------
✅ Проект полностью реализован и протестирован
✅ Готов к использованию
✅ Готов к публикации на GitHub

ЛИЦЕНЗИЯ:
---------
MIT License
