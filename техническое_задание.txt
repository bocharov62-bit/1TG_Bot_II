ТЕХНИЧЕСКОЕ ЗАДАНИЕ
TELEGRAM БОТ С ЛОКАЛЬНОЙ AI МОДЕЛЬЮ
===================================

1. ОБЩЕЕ ОПИСАНИЕ
-----------------
Название проекта: Telegram бот с локальной AI моделью
Версия: 1.0
Дата: 2025-12-27

Назначение: 
Разработка Telegram бота, который отвечает на вопросы пользователей 
с использованием локальной AI модели gemma3:1b через Ollama. Бот 
работает локально в Docker контейнерах и не требует облачных AI сервисов.

Целевая аудитория: 
Пользователи Telegram, желающие получить быстрые ответы на вопросы 
без отправки данных в облачные сервисы.

2. ФУНКЦИОНАЛЬНЫЕ ТРЕБОВАНИЯ
-----------------------------

2.1. Основной функционал:
  ✅ Прием текстовых запросов от пользователей через Telegram
  ✅ Генерация ответов с помощью локальной AI модели (gemma3:1b)
  ✅ Неформальный стиль общения
  ✅ Краткие ответы (1-2 предложения максимум)
  ✅ Приветствие при команде /start

2.2. Команды бота:
  - /start - приветствие с текстом "Задайте свой вопрос, и я постараюсь на него ответить."
  - Любое текстовое сообщение обрабатывается как запрос к AI

2.3. Обработка ошибок:
  - При недоступности Ollama: "Вернитесь позже, я пока занят."
  - При таймауте (> 30 сек): "Вернитесь позже, я пока занят."
  - При ошибках API: "Вернитесь позже, я пока занят."
  - Все ошибки логируются с детальной информацией

2.4. Логирование:
  - Все входящие сообщения
  - Все запросы к AI
  - Все ответы от AI
  - Все ошибки с полным traceback
  - Уровни: INFO, ERROR
  - Формат: [TIMESTAMP] [LEVEL] [MODULE] MESSAGE
  - Файлы: logs/bot_YYYY-MM-DD.log

3. ТЕХНИЧЕСКИЕ ТРЕБОВАНИЯ
--------------------------

3.1. Технологический стек:
  - Язык: Python 3.11+
  - Фреймворк бота: aiogram 3.13.1
  - AI сервис: Ollama (latest)
  - AI модель: gemma3:1b
  - HTTP клиент: aiohttp 3.9.5
  - Конфигурация: python-dotenv 1.0.1
  - Контейнеризация: Docker + Docker Compose

3.2. Архитектура:
  - Модульная структура проекта
  - Разделение на слои:
    * handlers/ - обработка сообщений Telegram
    * services/ - бизнес-логика (AI сервис)
    * utils/ - вспомогательные функции
  - Docker Compose с двумя сервисами:
    * ollama - AI сервис (порт 11434)
    * bot - Telegram бот (Python приложение)
  - Конфигурация через .env файл
  - Переменные окружения передаются в контейнеры

3.3. Структура проекта:
```
bot_primeta/
├── bot/
│   ├── handlers/      # Обработчики сообщений
│   ├── services/      # Сервисы (AI)
│   ├── utils/         # Утилиты
│   ├── config.py      # Конфигурация
│   └── main.py        # Точка входа
├── logs/              # Логи приложения
├── Dockerfile         # Образ для бота
├── docker-compose.yml # Оркестрация контейнеров
├── requirements.txt   # Python зависимости
├── .env               # Конфигурация (не в git)
├── env.example.txt    # Пример конфигурации
├── README.md          # Документация
└── QUICKSTART.md      # Быстрый старт
```

3.4. Конфигурация (.env):
  - TELEGRAM_BOT_TOKEN - токен бота от @BotFather
  - OLLAMA_URL - URL Ollama (http://ollama:11434 для Docker)
  - OLLAMA_MODEL - модель AI (gemma3:1b)
  - OLLAMA_TIMEOUT - таймаут запросов (30 сек)
  - LOG_LEVEL - уровень логирования (INFO)
  - LOG_DIR - директория для логов (logs)

3.5. Производительность:
  - Время ответа: < 30 секунд
  - Параллельная обработка запросов (asyncio)
  - Таймауты для предотвращения зависаний
  - Ограничение длины ответов AI

3.6. Безопасность:
  - Токен бота хранится в .env (не в git)
  - .env файл в .gitignore
  - env.example.txt без реальных данных
  - Локальная работа (данные не уходят в облако)

4. НЕФУНКЦИОНАЛЬНЫЕ ТРЕБОВАНИЯ
------------------------------

4.1. Надежность:
  - Автоматический перезапуск контейнеров (restart: unless-stopped)
  - Обработка всех исключений
  - Graceful shutdown при остановке
  - Логирование для диагностики проблем

4.2. Поддерживаемость:
  - Чистый, документированный код
  - Модульная структура
  - Комментарии в коде
  - README с инструкциями
  - Примеры конфигурации

4.3. Масштабируемость:
  - Легко заменить модель AI
  - Легко изменить параметры AI
  - Легко добавить новые команды

5. ОГРАНИЧЕНИЯ И ДОПУЩЕНИЯ
---------------------------

5.1. Ограничения:
  - Работает только локально (не 24/7 на сервере)
  - Зависимость от доступности Ollama
  - Качество ответов ограничено моделью gemma3:1b
  - Требует Docker для работы
  - Модель занимает ~815 MB на диске

5.2. Допущения:
  - Docker и Docker Compose установлены
  - Достаточно RAM (2-3 GB) для модели
  - Достаточно места на диске (~1 GB)
  - Ollama работает в Docker контейнере
  - Модель gemma3:1b загружена в Ollama
  - Пользователь имеет токен бота

6. КРИТЕРИИ ПРИЕМКИ
-------------------

6.1. Функциональность:
  ✅ Бот отвечает на команду /start приветствием
  ✅ Бот обрабатывает текстовые запросы
  ✅ Бот генерирует ответы через AI
  ✅ Ответы краткие (1-2 предложения)
  ✅ Стиль общения неформальный

6.2. Технические:
  ✅ Бот работает в Docker контейнере
  ✅ Ollama работает в отдельном контейнере
  ✅ Контейнеры общаются через Docker сеть
  ✅ Логирование работает корректно
  ✅ Обработка ошибок работает
  ✅ Проект готов для GitHub

6.3. Документация:
  ✅ README.md с описанием проекта
  ✅ QUICKSTART.md с быстрым стартом
  ✅ env.example.txt с примерами
  ✅ Комментарии в коде
  ✅ Техническое задание

7. ПЛАН РАЗРАБОТКИ
------------------

Этап 1: Подготовка инфраструктуры
  - Настройка Docker окружения
  - Создание структуры проекта
  - Настройка конфигурации

Этап 2: Разработка бота
  - Реализация обработчиков сообщений
  - Интеграция с Ollama
  - Обработка ошибок
  - Логирование

Этап 3: Тестирование
  - Тестирование основных сценариев
  - Тестирование обработки ошибок
  - Проверка производительности

Этап 4: Документация
  - Написание README
  - Создание примеров
  - Подготовка к публикации

8. СТАТУС ПРОЕКТА
-----------------
✅ Проект полностью реализован
✅ Все требования выполнены
✅ Протестирован и готов к использованию
✅ Готов к публикации на GitHub
